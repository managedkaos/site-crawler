# Site Crawler Examples

This directory contains example reports generated by Site Crawler to demonstrate its capabilities and different use cases.

## Example Reports

### 1. Simple Site Report - [`example_com-report.md`](example_com-report.md)

**Command:**

```bash
python main.py example.com --output example_com-report.md
```

**What it demonstrates:**

- **Basic crawling** of a simple, single-page website
- **Fast execution** (0.20 seconds) for sites with minimal content
- **Clean results** with no errors or issues
- **Single depth** crawling showing the base page only

**Key insights:**

- Example.com is a very simple site with just one page
- No internal links to follow, so crawling stops at depth 0
- Perfect for testing basic functionality

### 2. Error Reporting Example - [`google_com-report.md`](google_com-report.md)

**Command:**

```bash
python main.py google.com --output google_com-report.md
```

**What it demonstrates:**

- **Multi-depth crawling** (reached depth 3)
- **Error detection and reporting** (2 HTTP 404 errors found)
- **Comprehensive link discovery** (21 pages visited)
- **Real-world site complexity** handling

**Key insights:**

- Google.com has a complex structure with many internal links
- Found 2 broken links (404 errors):
  - `https://google.com/url`
  - `https://google.com/intl/ALL_us/accessible-features/`
- Shows how the crawler handles both HTTP and HTTPS protocols
- Demonstrates the detailed error reporting feature

**Error Analysis:**

- The crawler successfully identified and categorized HTTP 404 errors
- Error URLs are listed in the "DETAILED ERROR REPORT" section
- Shows the importance of error tracking for site maintenance

### 3. Interrupted Crawl Example - [`github_com-report.md`](github_com-report.md)

**Command:**

```bash
python main.py github.com --output github_com-report.md
```

**What it demonstrates:**

- **Partial report generation** when crawling is interrupted
- **Large site handling** (67 pages visited before interruption)
- **Graceful interruption handling** with progress preservation
- **Complex site structure** with many GitHub repositories

**Key insights:**

- GitHub.com is a massive site with thousands of pages
- The crawl was interrupted after 94.47 seconds and 67 pages
- Shows the "⚠️ **PARTIAL REPORT** - Crawling was interrupted" indicator
- Found 1 HTTP 400 error on the marketplace page
- Demonstrates the crawler's ability to handle very large sites

**Interruption Handling:**

- All progress up to the interruption point is preserved
- The report clearly indicates it's a partial result
- Useful for large sites where you might want to stop early

## Report Structure Analysis

All reports follow the same comprehensive structure:

### 1. Summary Metrics

- **Base URL and Domain**: The starting point and domain being crawled
- **Timing**: Start time and total duration
- **Statistics**: Total requests, pages visited, and max depth reached

### 2. HTTP Status Code Summary

- **Status code breakdown**: Count of each HTTP status code encountered
- **Error categorization**: Clear identification of error vs. success codes

### 3. Detailed Error Report (when errors exist)

- **Error categorization**: Grouped by HTTP status code
- **Specific URLs**: Exact URLs that returned errors
- **Actionable information**: URLs that need attention

### 4. All Visited Pages by Depth

- **Organized by depth**: Pages grouped by their crawl depth
- **Status indicators**: Each URL shows its HTTP status code
- **Complete audit trail**: Full list of all pages visited

## Use Cases Demonstrated

### 1. Site Health Auditing

The Google.com example shows how to audit a site for broken links and errors.

### 2. Site Structure Analysis

All examples demonstrate how to understand a site's structure and link relationships.

### 3. Performance Monitoring

The timing information helps identify slow-loading pages or sites.

### 4. Large Site Exploration

The GitHub.com example shows how to explore large sites with interruption handling.

### 5. Error Detection

All examples demonstrate the crawler's ability to detect and report various HTTP errors.

## Running Your Own Examples

To generate similar reports for your own sites:

```bash
# Basic crawl
python main.py yoursite.com --output yoursite-report.md

# Deep crawl with custom settings
python main.py yoursite.com --max-depth 5 --delay 0.5 --output deep-crawl-report.md

# Quick test crawl
python main.py yoursite.com --max-depth 1 --delay 0.1 --output quick-test.md
```

## Tips for Different Site Types

### Small Sites (like example.com)

- Use default settings
- Fast execution, minimal depth needed

### Medium Sites (like google.com)

- Default depth (3) usually sufficient
- Watch for error patterns
- Consider custom delays for rate limiting

### Large Sites (like github.com)

- Start with lower depth to test
- Be prepared for long execution times
- Use interruption handling for partial results
- Consider using `--max-depth 1` for initial exploration
